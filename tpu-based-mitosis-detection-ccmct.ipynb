{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":903990,"sourceType":"datasetVersion","datasetId":484457}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPU VM v3-8","metadata":{}},{"cell_type":"code","source":"!pip install pydicom\n!pip install pillow==10.0.0\n!pip install ipywidgets\n!pip install torch-xla\n!pip install pandas\n!pip install --upgrade pip\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:06.077662Z","iopub.execute_input":"2024-05-16T17:41:06.078647Z","iopub.status.idle":"2024-05-16T17:41:26.449960Z","shell.execute_reply.started":"2024-05-16T17:41:06.078608Z","shell.execute_reply":"2024-05-16T17:41:26.448970Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pydicom in /usr/local/lib/python3.10/site-packages (2.4.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pillow==10.0.0 in /usr/local/lib/python3.10/site-packages (10.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/site-packages (8.1.2)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.23.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.2)\nRequirement already satisfied: widgetsnbextension~=4.0.10 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\nRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.10.0)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: torch-xla in /usr/local/lib/python3.10/site-packages (2.2.0+libtpu)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch-xla) (1.4.0)\nRequirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/site-packages (from torch-xla) (0.10)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from torch-xla) (6.0.1)\nRequirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla) (1.8.0)\nRequirement already satisfied: oauth2client in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla) (4.1.3)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.22.0)\nRequirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.29.0)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.2.0)\nRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.34.1)\nRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.16.0)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.6.0)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.4.0)\nRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (4.9)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.63.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.31.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (5.3.3)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2024.2.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (24.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Restarts the Jupyter kernel","metadata":{}},{"cell_type":"code","source":"from IPython.display import display_html\ndef restartkernel() :\n    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:26.451626Z","iopub.execute_input":"2024-05-16T17:41:26.451885Z","iopub.status.idle":"2024-05-16T17:41:26.455682Z","shell.execute_reply.started":"2024-05-16T17:41:26.451859Z","shell.execute_reply":"2024-05-16T17:41:26.455091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"restartkernel()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:26.456411Z","iopub.execute_input":"2024-05-16T17:41:26.456649Z","iopub.status.idle":"2024-05-16T17:41:26.468512Z","shell.execute_reply.started":"2024-05-16T17:41:26.456627Z","shell.execute_reply":"2024-05-16T17:41:26.467859Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/html":"<script>Jupyter.notebook.kernel.restart()</script>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport xml.etree.ElementTree as ET\nimport torch\nimport torchvision\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_color_lut\nfrom pydicom.pixel_data_handlers.util import apply_modality_lut\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom PIL import Image\nimport torchvision.transforms as torchvision_transforms\nimport io\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport sqlite3\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-16T17:41:26.470152Z","iopub.execute_input":"2024-05-16T17:41:26.470408Z","iopub.status.idle":"2024-05-16T17:41:31.673222Z","shell.execute_reply.started":"2024-05-16T17:41:26.470384Z","shell.execute_reply":"2024-05-16T17:41:31.672150Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Initialize Path","metadata":{}},{"cell_type":"code","source":"path = Path('/kaggle/input/mitosis-wsi-ccmct-training-set/')","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.674420Z","iopub.execute_input":"2024-05-16T17:41:31.674789Z","iopub.status.idle":"2024-05-16T17:41:31.678899Z","shell.execute_reply.started":"2024-05-16T17:41:31.674763Z","shell.execute_reply":"2024-05-16T17:41:31.678208Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Initialize Database","metadata":{}},{"cell_type":"code","source":"database = sqlite3.connect(str(path/'MITOS_WSI_CCMCT_ODAEL_train_dcm.sqlite'))","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.680039Z","iopub.execute_input":"2024-05-16T17:41:31.680405Z","iopub.status.idle":"2024-05-16T17:41:31.699437Z","shell.execute_reply.started":"2024-05-16T17:41:31.680339Z","shell.execute_reply":"2024-05-16T17:41:31.698721Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Define output directories","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = '/kaggle/working/'  \nMODEL_DIR = os.path.join(OUTPUT_DIR, 'models')\nVISUALIZATION_DIR = os.path.join(OUTPUT_DIR, 'visualizations')\nPREDICTION_DIR = os.path.join(OUTPUT_DIR, 'predictions')\nREPORT_DIR = os.path.join(OUTPUT_DIR, 'reports')","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.700360Z","iopub.execute_input":"2024-05-16T17:41:31.700593Z","iopub.status.idle":"2024-05-16T17:41:31.704450Z","shell.execute_reply.started":"2024-05-16T17:41:31.700571Z","shell.execute_reply":"2024-05-16T17:41:31.703782Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Define utility functions","metadata":{}},{"cell_type":"code","source":"def create_output_directories(parent_dir, subdirs=['train', 'val', 'test']):\n    for subdir in subdirs:\n        dir_path = os.path.join(parent_dir, subdir)\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n            print(f\"Created directory: {dir_path}\")\n\ndef visualize_predictions(image, predictions, class_names, score_threshold=0.5):\n    fig, ax = plt.subplots(1)\n    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    image = image.numpy()\n    image = np.transpose(image, (1, 2, 0))\n    ax.imshow(image)\n\n    boxes = predictions['boxes'].cpu().numpy()\n    scores = predictions['scores'].cpu().numpy()\n    labels = predictions['labels'].cpu().numpy()\n\n    for box, score, label in zip(boxes, scores, labels):\n        if score > score_threshold:\n            xmin, ymin, xmax, ymax = box\n            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            ax.text(xmin, ymin, f'{class_names[label]}: {score:.2f}', color='r')\n\n    plt.axis('off')\n    plt.show()\n\ndef save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):\n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n    xm.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }, checkpoint_path)\n    print(f\"Saved checkpoint to: {checkpoint_path}\")\n\ndef load_checkpoint(model, optimizer, checkpoint_dir):\n    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_')]\n\n    if not checkpoint_files:\n        print(\"No checkpoints found in the directory.\")\n        return 0, None \n\n    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]), reverse=True)\n\n    latest_checkpoint_path = os.path.join(checkpoint_dir, checkpoint_files[0])\n    checkpoint = torch.load(latest_checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    print(f\"Loaded checkpoint from epoch {epoch} with loss {loss}\")\n    return epoch, loss","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.705500Z","iopub.execute_input":"2024-05-16T17:41:31.705751Z","iopub.status.idle":"2024-05-16T17:41:31.718137Z","shell.execute_reply.started":"2024-05-16T17:41:31.705728Z","shell.execute_reply":"2024-05-16T17:41:31.717445Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Define preprocessing functions","metadata":{}},{"cell_type":"code","source":"def apply_voi_lut(image, dataset, index=0):\n    if 'VOILUTSequence' in dataset:\n        return apply_lut(image, dataset.VOILUTSequence[index])\n    return image\n\ndef apply_modality_lut(image, dataset):\n    if 'ModalityLUTSequence' in dataset:\n        return apply_lut(image, dataset.ModalityLUTSequence[0])\n    return image\n\ndef apply_color_lut(image, dataset):\n    if 'LUTDescriptor' in dataset:\n        return apply_lut(image, dataset)\n    return image\n\ndef apply_lut(image, dataset,lut_descriptor=None):\n    if lut_descriptor is None:\n        lut_descriptor = dataset.LUTDescriptor\n    nr_entries = lut_descriptor[0] or 2**16\n    first_mapped = lut_descriptor[1]\n    bits = lut_descriptor[2]\n    output_range = 2**bits - 1\n    try:\n        lut_data = dataset.LUTData\n    except AttributeError:\n        return image\n    lut = np.zeros(nr_entries, dtype=np.uint32)\n    for i in range(0, len(lut_data), 2):\n        lut[i // 2] = lut_data[i] + (lut_data[i + 1] << 8)\n\n    if not np.array_equal(lut, np.arange(first_mapped, first_mapped + nr_entries, dtype=lut.dtype)):\n        clipped_img = np.clip(image - first_mapped, 0, nr_entries - 1)\n        return (lut[clipped_img] / output_range * 65535).astype(np.uint16)\n    return image\n\n\ndef apply_palette_color(image, dataset):\n    try:\n        lut = dataset.PaletteColorLookupTableDataArray\n\n        if lut is None or len(lut) == 0:\n            raise ValueError(\"No Palette Color Lookup Table found in the DICOM dataset\")\n\n        lut_entries = len(lut) // 3 \n        image = np.array(lut[image.flatten() * 3]).reshape(image.shape + (3,))\n\n        if image.dtype != np.uint8:\n            image = (image / image.max() * 255).astype(np.uint8)\n\n        return image\n    except Exception as e:\n        print(f\"Error applying palette color: {e}\")\n        return image \n\n\ndef normalize_dicom(image, ds):\n\n    photometric_interpretation = ds.PhotometricInterpretation\n    modality = ds.Modality\n\n    if photometric_interpretation == 'MONOCHROME1' or photometric_interpretation == 'MONOCHROME2':\n        image = apply_voi_lut(image, ds, index=0)\n        return apply_modality_lut(image, ds)\n\n    elif photometric_interpretation == 'RGB':\n        return apply_modality_lut(image,ds)\n\n    elif photometric_interpretation == 'YBR_FULL_422' or photometric_interpretation == 'YBR_FULL':\n        # Convert YBR to RGB\n        image = cv2.cvtColor(image, cv2.COLOR_YCrCb2RGB)\n        return apply_color_lut(image, ds)\n\n    elif photometric_interpretation == 'PALETTE COLOR':\n        image = apply_palette_color(image, ds)\n        return apply_color_lut(image, ds)\n\n    else:\n        print(f\"Warning: Unhandled Photometric Interpretation: {photometric_interpretation}\")\n        return image\n\ndef resize_image(image, target_size):\n    return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n\ndef save_patch(patch, output_path, filename):\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    full_path = os.path.join(output_path, filename)\n    cv2.imwrite(full_path, patch)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.719416Z","iopub.execute_input":"2024-05-16T17:41:31.719684Z","iopub.status.idle":"2024-05-16T17:41:31.733770Z","shell.execute_reply.started":"2024-05-16T17:41:31.719659Z","shell.execute_reply":"2024-05-16T17:41:31.733183Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Create output directories with subdirectories","metadata":{}},{"cell_type":"code","source":"create_output_directories(MODEL_DIR, subdirs=['train', 'val', 'test'])\ncreate_output_directories(VISUALIZATION_DIR, subdirs=['train', 'val', 'test'])\ncreate_output_directories(PREDICTION_DIR, subdirs=[])\ncreate_output_directories(REPORT_DIR, subdirs=[])","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.736106Z","iopub.execute_input":"2024-05-16T17:41:31.736416Z","iopub.status.idle":"2024-05-16T17:41:31.744922Z","shell.execute_reply.started":"2024-05-16T17:41:31.736390Z","shell.execute_reply":"2024-05-16T17:41:31.744339Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Define model architectures","metadata":{}},{"cell_type":"code","source":"def create_faster_rcnn_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.745735Z","iopub.execute_input":"2024-05-16T17:41:31.745995Z","iopub.status.idle":"2024-05-16T17:41:31.753940Z","shell.execute_reply.started":"2024-05-16T17:41:31.745950Z","shell.execute_reply":"2024-05-16T17:41:31.753326Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"class MitoticDataset(Dataset):\n    def __init__(self, slides, database, image_dir, transforms, patch_size=224):\n        self.slides = slides\n        self.database = database \n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.patch_size = patch_size\n        self.annotations = self.load_annotations()\n\n    def load_annotations(self):\n        all_annotations = []\n        for slide_filename in self.slides:\n            getannotations = f\"\"\"\n            SELECT \n                C.coordinateX, \n                C.coordinateY,\n                A.agreedClass\n            FROM Annotations AS A\n            JOIN Annotations_coordinates AS C ON A.uid = C.annoId\n            JOIN Slides ON A.slide = Slides.uid\n            WHERE Slides.filename = '{slide_filename[0]}' AND A.agreedClass = 2 \n            \"\"\"\n            annotations = self.database.execute(getannotations).fetchall()\n            for annotation in annotations:\n                all_annotations.append({\n                    'slide': slide_filename[0],\n                    'center_x': annotation[0],\n                    'center_y': annotation[1],\n                    'class': annotation[2],\n                })\n        return all_annotations\n\n    def __len__(self):\n        return len(self.slides)\n\n    def __getitem__(self, slide_idx):\n        slide_filename = self.slides[slide_idx][0]\n        slide_path = os.path.join(self.image_dir, slide_filename)\n\n        ds = pydicom.dcmread(slide_path)\n\n        tile_width = int(ds.Columns)\n        tile_height = int(ds.Rows)\n        num_frames = int(ds.NumberOfFrames)\n\n        patches = []\n        targets = []\n\n        for annotation in self.annotations:\n            if annotation['slide'] == slide_filename:\n                tile_x = annotation['center_x'] // tile_width\n                tile_y = annotation['center_y'] // tile_height\n\n                annotation_x_in_tile = annotation['center_x'] % tile_width\n                annotation_y_in_tile = annotation['center_y'] % tile_height\n\n                frame_index = tile_y * (ds.TotalPixelMatrixColumns // tile_width) + tile_x\n                if frame_index < num_frames:\n                    image = ds.pixel_array[frame_index]\n                else:\n                    print(f\"Error: Frame index {frame_index} out of bounds for {slide_filename}\")\n                    continue \n\n                x_start = max(0, annotation_x_in_tile - self.patch_size // 2)\n                y_start = max(0, annotation_y_in_tile - self.patch_size // 2)\n                x_end = min(image.shape[1], x_start + self.patch_size)\n                y_end = min(image.shape[0], y_start + self.patch_size)\n\n                patch = image[y_start:y_end, x_start:x_end]\n\n                patch = normalize_dicom(patch, ds)\n\n                if patch.shape[0] != self.patch_size or patch.shape[1] != self.patch_size:\n                    patch = resize_image(patch, (self.patch_size, self.patch_size))\n\n                half_patch = self.patch_size // 2\n                boxes = torch.tensor([[annotation_x_in_tile - half_patch,\n                                      annotation_y_in_tile - half_patch,\n                                      annotation_x_in_tile + half_patch,\n                                      annotation_y_in_tile + half_patch]], dtype=torch.float32)\n                labels = torch.tensor([1], dtype=torch.int64)\n                target = {\n                    'boxes': boxes,\n                    'labels': labels\n                }\n                \n                if self.transforms:\n                    patch = self.transforms(patch)\n                else:\n                    patch = self.transforms.ToTensor()(patch)\n                \n                patches.append(patch)\n                targets.append(target)\n\n        return patches, targets \n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.754893Z","iopub.execute_input":"2024-05-16T17:41:31.755132Z","iopub.status.idle":"2024-05-16T17:41:31.769394Z","shell.execute_reply.started":"2024-05-16T17:41:31.755109Z","shell.execute_reply":"2024-05-16T17:41:31.768703Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, device, epochs, batch_size, checkpoint_dir):\n    model.to(device)\n    model.train()\n\n    start_epoch = 0\n    start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_dir)\n\n    for epoch in range(start_epoch, epochs):\n        total_loss = 0.0\n        num_batches = 0\n\n        for slide_idx in tqdm(range(len(dataloader)), unit=\"slide\"):\n            patches, targets = dataloader.dataset[slide_idx]\n            for batch_start in range(0, len(patches), batch_size):\n                batch_end = min(batch_start + batch_size, len(patches))\n                images = torch.stack(patches[batch_start:batch_end]).to(device)\n                targets_batch = [{k: v.to(device) for k, v in t.items()} for t in targets[batch_start:batch_end]]\n\n                optimizer.zero_grad()\n                loss_dict = model(images, targets_batch)\n                losses = sum(loss for loss in loss_dict.values())\n\n                xm.optimizer_step(optimizer, barrier=True)\n                xm.mark_step()\n\n                total_loss += losses.item()\n                num_batches += 1\n\n        avg_loss = total_loss / num_batches\n        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n        save_checkpoint(model, optimizer, epoch + 1, avg_loss, checkpoint_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:41:31.770346Z","iopub.execute_input":"2024-05-16T17:41:31.770599Z","iopub.status.idle":"2024-05-16T17:41:31.783592Z","shell.execute_reply.started":"2024-05-16T17:41:31.770576Z","shell.execute_reply":"2024-05-16T17:41:31.782884Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Main execution","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    getslides = \"\"\"SELECT filename FROM Slides\"\"\"\n    all_slides = database.execute(getslides).fetchall()\n    \n    random.seed(42)\n    random.shuffle(all_slides) \n\n    train_split = int(0.6 * len(all_slides))\n    val_split = int(0.8 * len(all_slides))\n\n    train_slides = all_slides[:train_split]\n    val_slides = all_slides[train_split:val_split]\n    test_slides = all_slides[val_split:]\n\n    print(f\"Number of training slides: {len(train_slides)}\")\n    print(f\"Number of validation slides: {len(val_slides)}\")\n    print(f\"Number of test slides: {len(test_slides)}\")\n\n    image_dir = str(path) \n    \n    print(f\"# --- Data Transforms ---\")\n\n    data_transforms = torchvision_transforms.Compose([\n        torchvision_transforms.ToTensor(),\n        torchvision_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n    ])\n\n    print(f\"# --- Create Datasets ---\")\n    train_dataset = MitoticDataset(train_slides, database, image_dir, transforms=data_transforms)\n    val_dataset = MitoticDataset(val_slides, database, image_dir, transforms=data_transforms)\n    test_dataset = MitoticDataset(test_slides, database, image_dir, transforms=data_transforms)\n\n    print(f\"# --- Create DataLoaders ---\")\n    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=4) \n    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=4)\n    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=4)\n\n    print(f\"# --- Model---\")\n    device = xm.xla_device() #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    num_classes = 2 \n    model = create_faster_rcnn_model(num_classes).to(device)\n  \n    print(f\"# --- Optimizer ---\")\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    print(f\"# --- Directory to store checkpoints ---\")\n    checkpoint_dir = os.path.join(MODEL_DIR, 'train')\n    print(checkpoint_dir)\n\n    print(f\"# --- Training ---:{device}\")\n    batch_size = 4\n    train_model(model, train_dataloader, optimizer, device, epochs=20, batch_size=batch_size, checkpoint_dir=checkpoint_dir)\n    \n    create_output_directories(os.path.join(OUTPUT_DIR, 'predictions'), subdirs=[])\n    create_output_directories(os.path.join(OUTPUT_DIR, 'reports'), subdirs=[])\n          \n    print(f\"# --- Visualization --- \")\n    \n    print(f\"# Move images and targets to the device \")\n    images, targets = next(iter(test_dataloader))\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n\n    \n    print(f\"# --- Load the latest model for Inference ---\")\n    model_inference = create_faster_rcnn_model(num_classes).to(device)\n    _, _ = load_checkpoint(model_inference, optimizer, checkpoint_dir)  # Fixed line\n    model_inference.eval()\n    \n    print(f\"# Get model predictions \")\n    all_predictions = []\n    with torch.no_grad():\n        for slide_idx in tqdm(range(len(test_dataloader)), unit=\"slide\"):\n            patches, _ = test_dataloader.dataset[slide_idx]\n\n            for i in range(0, len(patches), batch_size):\n                batch_end = min(i + batch_size, len(patches))\n                image_batch = torch.stack(patches[i:batch_end]).to(device)\n                predictions = model_inference(image_batch) \n                all_predictions.extend(predictions) \n    \n        \n    print(f\"# --- Process and Analyze Predictions ---\")   \n    for slide_idx, predictions in enumerate(all_predictions):\n        print(f\"Predictions for Slide {slide_idx + 1}:\")\n        boxes = predictions['boxes'].cpu().numpy()\n        scores = predictions['scores'].cpu().numpy()\n        labels = predictions['labels'].cpu().numpy()\n\n        confident_mask = scores > 0.5\n        confident_boxes = boxes[confident_mask]\n        confident_scores = scores[confident_mask]\n        confident_labels = labels[confident_mask]\n\n        num_mitotic_figures = np.sum(confident_labels == 1)\n        print(f\"  Number of mitotic figures: {num_mitotic_figures}\")\n\n        visualize_predictions(patches[slide_idx].cpu().permute(1, 2, 0),\n                             predictions, \n                             class_names=['background', 'mitotic figure'])  \n\n        prediction_df = pd.DataFrame({\n            'xmin': confident_boxes[:, 0],\n            'ymin': confident_boxes[:, 1],\n            'xmax': confident_boxes[:, 2],\n            'ymax': confident_boxes[:, 3],\n            'score': confident_scores,\n            'label': confident_labels\n        })\n        prediction_df.to_csv(os.path.join(OUTPUT_DIR, 'predictions', f'slide_{slide_idx+1}_predictions.csv'), index=False)\n        \n        total_mitotic_figures += num_mitotic_figures\n\n        plt.figure()\n        plt.hist(confident_scores, bins=10, range=(0.0, 1.0))\n        plt.title(f'Confidence Score Histogram - Slide {slide_idx + 1}')\n        plt.xlabel('Confidence Score')\n        plt.ylabel('Frequency')\n        plt.savefig(os.path.join(OUTPUT_DIR, 'visualizations', f'slide_{slide_idx + 1}_score_histogram.png'))\n        plt.close()\n\n        with open(os.path.join(OUTPUT_DIR, 'reports', f'slide_{slide_idx + 1}_report.txt'), 'w') as f:\n            f.write(f\"Slide: {slide_filename}\\n\")\n            f.write(f\"Number of Mitotic Figures: {num_mitotic_figures}\\n\")\n            f.write(f\"Precision: {precision:.4f}\\n\")\n            f.write(f\"Recall: {recall:.4f}\\n\")\n            f.write(f\"mAP: {mAP:.4f}\\n\")\n    ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-16T17:41:31.784660Z","iopub.execute_input":"2024-05-16T17:41:31.784932Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Number of training slides: 12\nNumber of validation slides: 4\nNumber of test slides: 5\n# --- Data Transforms ---\n# --- Create Datasets ---\n","output_type":"stream"}]}]}